else
blong[i] <- 2
}
blong
}
discriminiant.fisher(iris[1:25,1:4],iris[51:75,1:4],iris[c(1:100),1:4])
##Naive Bayes classifier
#install.packages("e1071")
library(e1071)
tr <- sample(1:50, 25)
train <- rbind(iris3[tr,,1], iris3[tr,,2], iris3[tr,,3])
test <- rbind(iris3[-tr,,1], iris3[-tr,,2], iris3[-tr,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
iris.nb<-naiveBayes(train, cl)
iris.fit<-predict(iris.nb, train)
table(iris.fit,cl)
#test set
iris.pred<-predict(iris.nb,test)
table(iris.pred,cl)
library(class)
n <- 200
x1 <- cbind(1+rnorm(3*n,sd=0.3),rep(1:3,each=n)+rnorm(3*n,sd=0.3),c(rep(1,n),rep(2,n),rep(1,n)))
x2 <- cbind(2+rnorm(3*n,sd=0.3),rep(1:3,each=n)+rnorm(3*n,sd=0.3),c(rep(2,n),rep(3,n),rep(2,n)))
x3 <- cbind(3+rnorm(3*n,sd=0.3),rep(1:3,each=n)+rnorm(3*n,sd=0.3),c(rep(3,n),rep(1,n),rep(3,n)))
x<-rbind(x1,x2,x3)
plot(x[,1:2],col=x[,3])
train<-rbind(x1[c(1:100,201:300,401:500),],x2[c(1:100,201:300,401:500),],x3[c(1:100,201:300,401:500),])
cl<-train[,3]
train<-train[,-3]
plot(train,col=as.numeric(cl))
test<-rbind(x1[c(101:200,301:400,501:600),],x2[c(101:200,301:400,501:600),],x3[c(101:200,301:400,501:600),])
tcl<-test[,3]
test<-test[,-3]
cl.knn<-knn(train,test,cl,k=3,prob=T,use.all=T)
table(cl.knn)
classError(cl.knn,tcl)
#cv for selection k
cverr<-rep(0,50)
for(i in 1:50){
set.seed(i)  # set the seed
kcl<-knn(train,test,cl,k=i,use.all=T)
cverr[i]<-classError(kcl,cl)$errorRate
}
plot(1:50,cverr,type="l")
kk<-which.min(cverr)
kk
cverr[kk]
set.seed(kk)
points(kk,cverr[kk],col=2,pch=15)
cl.knn<-knn(train,test,cl,k=kk,prob=T,use.all=T)
table(cl.knn)
classError(cl.knn,tcl)
library(RColorBrewer)
x1<-train[,1]
y1<-train[,2]
mycols <- brewer.pal(8, "Dark2")[c(1,3,2)]
cols <- as.numeric(cl)
GS<-80
XL<-range(x1)
tmpx<-seq(XL[1],XL[2],len=GS)
YL<-range(y1)
tmpy<-seq(YL[1],YL[2],len=GS)
newx<-expand.grid(tmpx,tmpy)
yhat<-knn(train,newx,cl,k=kk)
colshat <- mycols[yhat]
plot(train,xlab="X",ylab="Y",xlim=XL,ylim=YL, main=
"kk-nearest neighbour", type="n")
points(newx,col=colshat,pch=".")
contour(tmpx,tmpy,matrix(yhat,GS,GS),levels=c(1:9),add=T,labels="", xlab="", ylab="",axes=FALSE,lwd=2)
points(train,col=cols)
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
knn(train, test, cl, k = 3)
cverr<-rep(0,24)
for(i in 1:24){
kcl<-knn(train,test,cl,k=i)
cverr[i]<-classError(kcl,cl)$errorRate
}
plot(1:24,cverr,type="l")
kk<-which.min(cverr)
cl.knn<-knn(train,test,cl,k=kk,prob=T)
table(cl.knn)
classError(cl.knn,cl)
iris.pc<-princomp(train,cor=T)
active_func <- function(x){
# sigmoid function
val_f = 1/(1 + exp(-x))
val_df = exp(-x)/(1 + exp(-x))^2
# ReLU function
# val_f = x*(x > 0)
# val_df = as.numeric(x > 0)
list(val_f = val_f, val_df = val_df)
}
forward_step <- function(y, w, b){
z = y%*%w + matrix(rep(b,batchsize),nrow = batchsize, byrow = TRUE)
val = active_func(z)
list(val_f = val$val_f, val_df = val$val_df)
}
backward_step <- function(delta, w, df){
# delta at layer N, of batchsize x layersize(N))
# w between N-1 and N [layersize(N-1) x layersize(N) matrix]
# df = df/dz at layer N-1, of batchsize x layersize(N-1)
return(delta%*%t(w)*df) # * means product by numbers
}
apply_nn <- function(y_input){
# use global variables
# Weights, Biases, NumLayers
# y_layer, df_layer for storing x-values and df/dz values
y = y_input
y_layer[[1]] <<- y
for(j in 1:NumLayers){
temp_val = forward_step(y, w = Weights[[j]], b = Biases[[j]])
y = temp_val$val_f
df = temp_val$val_df
df_layer[[j]] <<- df
y_layer[[j+1]] <<- y
}
return(y)
}
apply_nn_simple <- function(y_input){
# no storage for backprop (this is used for simple tests)
y = y_input
#y_layer[1] = y
for (j in 1:NumLayers) {
temp_val = forward_step(y, w = Weights[[j]], b = Biases[[j]])
y = temp_val$val_f
df = temp_val$val_df
}
return(y)
}
backprop <- function(y_target){
# the result will be the 'dw_layer' matrices that contain
# the derivatives of the cost function with respect to
# the corresponding weight
# use global variables
# y_layer, df_layer, Weights, Biases, NumLayers
# dw_layer, db_layer # dCost/dw and dCost/db (w,b=weights,biases)
# batchsize
delta = (y_layer[[NumLayers+1]]-y_target)*df_layer[[NumLayers]]
dw_layer[[NumLayers]] <<- (t(y_layer[[NumLayers]])%*%delta)/batchsize
db_layer[[NumLayers]] <<- colSums(delta)/batchsize
for (j in 1:(NumLayers-1)) {
delta = backward_step(delta, Weights[[NumLayers+1-j]], df_layer[[NumLayers-j]])
dw_layer[[NumLayers-j]] <<- (t(y_layer[[NumLayers-j]])%*%delta)/batchsize
db_layer[[NumLayers-j]] <<- colSums(delta)/batchsize
}
}
gradient_step <- function(eta){
# update weights & biases (after backprop!)
# use global variables
# dw_layer, db_layer, Weights, Biases
for (j in 1:NumLayers) {
Weights[[j]] <<- Weights[[j]] - eta*dw_layer[[j]]
Biases[[j]] <<- Biases[[j]] - eta*db_layer[[j]]
}
}
train_nn <- function(y_input, y_target, eta){
# y_in is an array of size batchsize x (input-layer-size)
# y_target is an array of size batchsize x (output-layer-size)
# eta is the stepsize for the gradient descent
# use global variables y_output
y_output <<- apply_nn(y_input)
backprop(y_target)
gradient_step(eta)
cost_result = 0.5*sum((y_target - y_output)^2)/batchsize
return(cost_result)
}
# Initialize global variables
NumLayers = 5 # does not count input-layer (but does count output)
LayerSizes = c(2,30,30,30,30,1) # input-layer,hidden-1,hidden-2,...,output-layer
batchsize=1000 # define the batchsize
Weights = c()
Biases = c()
y_layer = c()
df_layer = c()
dw_layer = c()
db_layer = c()
for (j in 1:NumLayers) {
Weights_Size = LayerSizes[j]*LayerSizes[j+1]
Weights[[j]] = matrix(runif(Weights_Size, -0.5, 0.5), nrow = LayerSizes[j])
Biases[[j]] = as.matrix(rep(0, LayerSizes[j+1]))
df_layer[[j]] = as.matrix(rep(0, LayerSizes[j+1]))
dw_layer[[j]] = matrix(0, nrow = LayerSizes[j], ncol = LayerSizes[j+1])
db_layer[[j]] = as.matrix(rep(0, LayerSizes[j+1]))
}
for (j in 1:(NumLayers+1)) {
y_layer[[j]] = as.matrix(rep(0, LayerSizes[j]))
}
# the function we want to have (desired outcome)
myfunc <- function(x0, x1){
q = x0^2+x1^2
val = exp(-5*q)*abs(x0+x1)
return(val)
}
x0 = seq(-0.5,0.5,length.out = 40)
x1 = seq(-0.5,0.5,length.out = 40)
r1 = outer(x0^2, x1^2, "+")
r2 = outer(x0, x1, "+")
plot_z <- exp(-5*r1)*abs(r2)
image(z = plot_z, col = heat.colors(40))
contour(z = plot_z)
# pick 'batchsize' random positions in the 2D square
make_batch <- function(){
# use global variable batchsize
inputs = matrix(runif(batchsize*2,-0.5,0.5), nrow = batchsize)
targets = as.matrix(rep(0,batchsize))
targets[,1] = myfunc(inputs[,1], inputs[,2])
return(list(inputs = inputs, targets = targets))
}
# training nn and plot the cost function
eta = 0.1
batches = 2000
costs = rep(0, batches)
for (j in 1:batches) {
temp_val = make_batch()
y_input = as.matrix(temp_val$inputs)
y_target = as.matrix(temp_val$targets)
costs[j] = train_nn(y_input, y_target, eta = 0.1)
cat("step: ", j, "\n")
}
library(Rcpp) # Attach R package `Rcpp'
# Define function `add'
cppFunction('int add(int x, int y, int z) {
int sum = x + y + z;
return sum;
}')
add(1,2,3)
install.packages("rtools")
system('g++ -v')
system('where make')
m <- 10000
burn.in <- 2000
sizes <- c(125,18,20,34)
size <- sum(sizes)
prob <- function(theta){
p <- c(0.5+theta/4,(1-theta)/4,(1-theta)/4,theta/4)
}
prob.ratio <- function(n,d){
prod(prob(n)^sizes/prob(d)^sizes)
}
# random walk
# using unif(-0.25,0.25) as step
x.rw <- numeric(m)
k.rw <- 0
u <- runif(m)
v <- runif(m,-0.25,0.25)
x.rw[1] <- v[1]
for (i in 2:m){
xt <- x.rw[i-1]
y <- xt + v[i]
r <- min(prob.ratio(y,xt),1)
if(!is.nan(r) && u[i] <= r){
x.rw[i] <- y
}else{
k.rw <- k.rw + 1
x.rw[i] <- xt
}
}
print(k.rw)
# MH samplings
sd <- 0.5
min <- -0.8
max <- 0.8
rg <- function(p){
return(runif(1,min-abs(p),max+abs(p)))
}
dg <- function(x,p){
return(dunif(x,min-abs(p),max+abs(p)))
}
x.mh <- numeric(m)
k.mh <- 0
u <- runif(m)
x.mh[1] <- rg(0)
for(i in 2:m){
xt <- x.mh[i-1]
y <- rg(xt)
r <- min(prob.ratio(y,xt)*dg(xt,y)/dg(y,xt),1)
if(!is.na(r) && u[i] <= r){
x.mh[i] <- y
}else{
x.mh[i] <- xt
k.mh <- k.mh + 1
}
}
print(k.mh)
# independence sampler
x.i <- numeric(m)
k.i <- 0
x.i[i] <- rg(0)
u <- runif(m)
for(i in 2:m){
xt <- x.i[i-1]
y <- rg(0)
r <- prob.ratio(y,xt)*dg(xt,0)/dg(y,0)
if(u[i] <= r){
x.i[i] <- y
}else{
x.i[i] <- xt
k.i <- k.i + 1
}
}
print(k.i)
#histogram
par(mfrow=c(3,2))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.i[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
par(mfrow=c(1,1))
par(mfcol=c(12,12), oma=c(1,1,0,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
for(m in 1:141 ){
x <- rnorm(100)
hist(x[x != 0],30, xlab=NA, ylab=NA, main=paste('data: ',m),
cex.axis=0.5, font.main=1, cex.main=0.8)
}
par(mfcol=c(2,2), oma=c(1,1,0,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
for(m in 1:4 ){
x <- rnorm(100)
hist(x[x != 0],30, xlab=NA, ylab=NA, main=paste('data: ',m),
cex.axis=0.5, font.main=1, cex.main=0.8)
}
#histogram
par(mfcol=c(3,2), oma=c(1,1,0,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.i[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
#histogram
par(mfcol=c(3,2), oma=c(1,0,1,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
par(mfcol=c(3,2))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
#histogram
par(mfrow=c(3,2), oma=c(1,0,1,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
#histogram
par(mfrow=c(3,2),oma=c(1,0,1,0), mar=c(1,1,1,0), tcl=-0.1, mgp=c(0,0,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.i[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
par(mfrow=c(1,1))
#histogram
par(mfrow=c(3,2),oma=c(1,0,1,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.i[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
#histogram
par(mfrow=c(3,2),oma=c(1,1,1,0))
is <- (burn.in + 1):m
xs <- as.list(x.rw,x.mh)
x <- x.rw[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.mh[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
x <- x.i[is]
hist(x,probability=TRUE)
plot(is,x,type="l")
par(mfrow=c(1,1))
sample.chain <- function(N, start){
#generates a Metropolis chain for multinomial distribution
#with random walk proposal distribution and starting value start
x <- rep(0, N)
x[1] <- start
u <- runif(N)
v <- runif(N,-0.25,0.25)
prob <- function(theta){
p <- c(0.5+theta/4,(1-theta)/4,(1-theta)/4,theta/4)
}
prob.ratio <- function(n,d){
prod(prob(n)^sizes/prob(d)^sizes)
}
for (i in 2:N){
xt <- x[i-1]
y <- xt + v[i] #candidate point
r <- min(prob.ratio(y,xt),1)
if (!is.nan(r) && u[i] <= r){
x[i] <- y
}else{
x[i] <- xt
}
}
return(x)
}
rnR <- sample.chain(100,1)
plot(rnR,type='l')
rnR <- sample.chain(1000,1)
plot(rnR,type='l')
Gelman.Rubin <- function(X) {
for (i in 1:nrow(psi))
}
Gelman.Rubin <- function(X) {
psi <- t(apply(X, 1, cumsum))#compute diagnostic statistics
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}
k <- 4 #number of chains to generate
n <- 15000 #length of chains
b <- 1000 #burn-in length
sizes <- c(125,18,20,34)
size <- sum(sizes)
x0 <- c(0.2, 0.4, 0.6, 0.8)#choose overdispersed initial values
X <- matrix(0, nrow=k, ncol=n)#generate the chains
for (i in 1:k){X[i, ] <- sample.chain(n, x0[i])}
print(Gelman.Rubin(X))
Gelman.Rubin(X)
theta = 1
eta = 0
N = 10000
stopifnot(theta > 0)
df = function(x) {
1/(theta*pi*(1+((x-eta)/theta)^2))
}
dg = function(x, df) {
# dt(x = x, df = df)
dnorm(x = x, mean = df)
}
rg = function(df) {
rnorm(n = 1, mean = df)
# rt(n = 1, df = df)
}
mh = function (N, df, dg, rg) {
x = numeric(N)
x[1] = rg(1)
k = 0
u = runif(N)
for (i in 2:N) {
xt = x[i-1]
y = rg(xt)
r = df(y) * dg(xt, y) / (df(xt) * dg(y, xt))
if (u[i] <= r) {
x[i] = y
} else {
k = k + 1
x[i] = xt
}
}
print(k)
return(x)
}
x = mh(N, df, dg, rg)
is = 1001:N
par(mfrow = c(1,2))
plot(is, x[is], type="l")
hist(x, probability = TRUE, breaks = 100)
plot.x = seq(min(x), max(x), 0.01)
lines(plot.x, df(plot.x))
par(mfrow = c(1,1))
help(rnorm)
help("rcauchy")
help(rnorm)
devtools::document()
devtools::document()
setwd("F:/Rproj/StatComp18087/R")
devtools::document()
